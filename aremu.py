# -*- coding: utf-8 -*-
"""Aremu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U9f9mePq9KaDHwhwbFfYXx5h5npB5NNr
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_excel('Rworkdd.xlsx')
df.head()

df.isna().sum()

df.info()

df.describe()

X = pd.get_dummies(df.drop('ALLERGY_HISTORY', axis=1),drop_first=True)
y = df['ALLERGY_HISTORY']

from sklearn.model_selection import train_test_split, GridSearchCV

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=101)

from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier

#help(RandomForestClassifier)

Rmodel = RandomForestClassifier(n_estimators=20,random_state=101,max_features='auto')

Rmodel.fit(X_train,y_train)

Rpreds = Rmodel.predict(X_test)

from sklearn.metrics import classification_report,confusion_matrix,ConfusionMatrixDisplay,accuracy_score,mean_absolute_error,mean_squared_error

cm = confusion_matrix(y_test,Rpreds)
dip=ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=Rmodel.classes_)
dip.plot()

#help(ConfusionMatrixDisplay)

confusion_matrix(y_test,Rpreds)

Rmodel.feature_importances_

print(classification_report(y_test,Rpreds))

sns.countplot(df,x='ALLERGY_HISTORY',hue='GENDER')

n_estimators=[64,100,128,200]
max_features= [8,13,20,28]
bootstrap = [True,False]
oob_score = [True,False]

param_grid = {'n_estimators':n_estimators,
             'max_features':max_features,
             'bootstrap':bootstrap,
             'oob_score':oob_score}  # Note, oob_score only makes sense when bootstrap=True!

rfc = RandomForestClassifier()
grid = GridSearchCV(rfc,param_grid)

grid.fit(X_train,y_train)

grid.best_params_

predicitions = grid.predict(X_test)

print(classification_report(y_test,predicitions))

confusion_matrix(
    y_test,predicitions)



"""# **ADA BOOST SECTION**"""

Adamodel = AdaBoostClassifier()

Adamodel.fit(X_train, y_train)

pdt = Adamodel.predict(X_test)

print(classification_report(y_test,pdt))

confusion_matrix(y_test,pdt)

error_rates = []

for n in range(1,96):

    model = AdaBoostClassifier(n_estimators=n)
    model.fit(X_train,y_train)
    preds = model.predict(X_test)
    err = 1 - accuracy_score(y_test,preds)

    error_rates.append(err)

plt.plot(range(1,96),error_rates)

model

model.feature_importances_

feats = pd.DataFrame(index=X.columns,data=model.feature_importances_,columns=['Importance'])

imp_feats = feats[feats['Importance']>0.03]
imp_feats

X.columns

imp_feats = imp_feats.sort_values("Importance")
plt.figure(figsize=(14,6),dpi=200)
sns.barplot(data=imp_feats.sort_values('Importance'),x=imp_feats.sort_values('Importance').index,y='Importance')

plt.xticks(rotation=90);

gang = [('random _Forrest',Rmodel),('AdaBoost',Adamodel)]
combined_model = VotingClassifier(estimators=gang, voting='soft')
combined_model.fit(X_train,y_train)

predC= combined_model.predict(X_test)
accuracy_score(y_test,predC)

ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test,predC)).plot()



iris = sns.load_dataset('iris')
iris.head()

from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import LabelEncoder

df.head()

#help(LabelEncoder)

dfcopy=df.copy()

dfcopy['ALLERGY_HISTORY']= LabelEncoder().fit_transform(dfcopy['ALLERGY_HISTORY'])

XX_train, XX_test,yy_train,yy_test= train_test_split(X,dfcopy['ALLERGY_HISTORY'],test_size=0.3, random_state=101)

clf=MultinomialNB()

clf.fit(XX_train,yy_train)

predNB = clf.predict(XX_test)

print(classification_report(yy_test,predNB))

ang = [('random _Forrest',Rmodel),('AdaBoost',Adamodel),('Naive Bayes', clf)]
bad = VotingClassifier(estimators=ang, voting='soft')
bad.fit(X_train,yy_train)

#predBad= bad.predict(X_test)
#accuracy_score(y_test,predBad)



"""**Combineing the 3 from start**"""

df.head()

df= df.drop('ID',axis=1)
df.head()

df['ALLERGY_HISTORY']=LabelEncoder().fit_transform(df['ALLERGY_HISTORY'])
#df['AGE']=pd.to_numeric(df['AGE'],errors='coerce').astype(int)

df = df.drop(df[df['AGE']=='BARBER'].index)

df['AGE']=pd.to_numeric(df['AGE'],errors='coerce').astype(int)
df.info()

X=pd.get_dummies(df.drop('ALLERGY_HISTORY', axis=1),drop_first=True)
y = df['ALLERGY_HISTORY']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
X_validation, X_holdouttest, y_validation, y_holdouttest = train_test_split(X_test, y_test, test_size=0.5, random_state=42)

len(X_validation)

model1 = MultinomialNB()
model2 = RandomForestClassifier(n_estimators=100, random_state=42)
model3 = AdaBoostClassifier(n_estimators=100, random_state=42)

model1.fit(X_train, y_train)
model2.fit(X_train, y_train)
model3.fit(X_train, y_train)

predictions1 = model1.predict(X_validation)
predictions2 = model2.predict(X_validation)
predictions3 = model3.predict(X_validation)



ng = [('random _Forrest',model2),('AdaBoost',model3),('Naive Bayes', model1)]
badder = VotingClassifier(estimators=ng, voting='soft')
badder.fit(X_train,y_train)

predbadder = badder.predict(X_test)
print(classification_report(y_test,predbadder))

newdataPred = badder.predict(X_holdouttest)
print(classification_report(y_holdouttest,newdataPred))

confusion_matrix(y_holdouttest,newdataPred)

mean_absolute_error(y_holdouttest,newdataPred)

mean_absolute_error(y_test, predbadder)

confusion_matrix(y_test,predbadder)



# prompt: how to save my ipynb file to my github repository

from google.colab import drive
drive.mount('/content/drive')

# Save the notebook to your Google Drive
!cp /content/Rworkdd.ipynb /content/drive/MyDrive/Rworkdd.ipynb

# Clone your GitHub repository
!git clone https://github.com/[your-username]/[your-repository].git

# Copy the notebook from your Google Drive to your local machine
!cp /content/drive/MyDrive/Rworkdd.ipynb /content/[your-repository]/

# Add the notebook to your GitHub repository
!git add /content/[your-repository]/Rworkdd.ipynb

# Commit the changes to your GitHub repository
!git commit -m "Added Rworkdd.ipynb"

# Push the changes to your GitHub repository
!git push origin master

